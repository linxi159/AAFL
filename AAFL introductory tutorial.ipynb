{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AAFL\n",
    "\n",
    "In this introductory tutorial, we will analyze a published human melanoma dataset (GSE72056) using AAFL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loda packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np;\n",
    "from torch.autograd import Variable\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, accuracy_score\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from sklearn import mixture\n",
    "import datetime\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load GPU environment [optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optim(object):\n",
    "    def _makeOptimizer(self):\n",
    "        if self.method == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.params, lr=self.lr, weight_decay = self.weight_decay)\n",
    "        elif self.method == 'adagrad':\n",
    "            self.optimizer = optim.Adagrad(self.params, lr=self.lr, weight_decay = self.weight_decay)\n",
    "        elif self.method == 'adadelta':\n",
    "            self.optimizer = optim.Adadelta(self.params, lr=self.lr, weight_decay = self.weight_decay)\n",
    "        elif self.method == 'adam':\n",
    "            self.optimizer = optim.Adam(self.params, lr=self.lr, weight_decay = self.weight_decay)\n",
    "        elif self.method == 'adamW':\n",
    "            self.optimizer = optim.AdamW(self.params, lr=self.lr, weight_decay = self.weight_decay)\n",
    "        else:\n",
    "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
    "\n",
    "    def __init__(self, params, method, lr, max_grad_norm, lr_decay=1, start_decay_at=None, weight_decay = 0.):\n",
    "        self.params = list(params)  # careful: params may be a generator\n",
    "        self.last_ppl = None\n",
    "        self.lr = lr\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.method = method\n",
    "        self.lr_decay = lr_decay\n",
    "        self.start_decay_at = start_decay_at\n",
    "        self.start_decay = False\n",
    "        self.weight_decay = weight_decay;\n",
    "\n",
    "        self._makeOptimizer()\n",
    "\n",
    "    def step(self):\n",
    "        # Compute gradients norm.\n",
    "        grad_norm = 0\n",
    "        for param in self.params:\n",
    "            grad_norm += math.pow(param.grad.data.norm(), 2)\n",
    "\n",
    "        grad_norm = math.sqrt(grad_norm)\n",
    "      \n",
    "        if grad_norm > 0:\n",
    "            shrinkage = self.max_grad_norm / grad_norm\n",
    "        else:\n",
    "            shrinkage = 1.\n",
    "\n",
    "        for param in self.params:\n",
    "            if shrinkage < 1:\n",
    "                param.grad.data.mul_(shrinkage)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        return grad_norm\n",
    "\n",
    "    # decay learning rate if val perf does not improve or we hit the start_decay_at limit\n",
    "    def updateLearningRate(self, ppl, epoch):\n",
    "        if self.start_decay_at is not None and epoch >= self.start_decay_at:\n",
    "            self.start_decay = True\n",
    "        if self.last_ppl is not None and ppl > self.last_ppl:\n",
    "            self.start_decay = True\n",
    "\n",
    "        if self.start_decay:\n",
    "            self.lr = self.lr * self.lr_decay\n",
    "            print(\"Decaying learning rate to %g\" % self.lr)\n",
    "        #only decay for one epoch\n",
    "        self.start_decay = False\n",
    "        self.last_ppl = ppl\n",
    "        self._makeOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature reconstruction network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FR_Model(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(FR_Model, self).__init__()\n",
    "        self.pre_win = args.pre_win\n",
    "        self.m = args.m\n",
    "        self.p_list = (args.p_list) \n",
    "        self.len_p_list = len(args.p_list) \n",
    "        self.compress_p_list = args.compress_p_list\n",
    "        self.p_allsum = np.sum(self.p_list)\n",
    "        self.len_compress_p_list = len(self.compress_p_list)\n",
    "        if self.len_compress_p_list>0:\n",
    "            \n",
    "            self.compress_p = args.compress_p_list[-1]\n",
    "            self.weight = nn.Parameter(1e-10*torch.ones([self.m, self.compress_p, self.pre_win]))\n",
    "        else:\n",
    "            self.weight = nn.Parameter(1e-10*torch.ones([self.m, self.p_allsum, self.pre_win]))\n",
    "        self.bias = nn.Parameter(1e-12*torch.ones(self.m,self.pre_win)) \n",
    "                \n",
    "    def forward(self, x):\n",
    "        if self.pre_win ==1:\n",
    "            final_y = torch.empty(x.shape[0], self.m) \n",
    "        else :\n",
    "            final_y = torch.empty(x.shape[0], self.pre_win, self.m) \n",
    "        \n",
    "        for j in range(self.m):           \n",
    "            if self.pre_win ==1:   \n",
    "                final_y[:,j] = F.linear(x[:,j,:], self.weight[j,:].view(1, self.weight.shape[1]), self.bias[j,:]).view(-1);               \n",
    "            else:\n",
    "                final_y[:,:,j] = F.linear(x[:,j,:], self.weight[j,:].transpose(1,0), self.bias[j,:]);                       \n",
    "        return final_y;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define low-rank network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_Model(nn.Module):\n",
    "    def __init__(self, in_features, low_rank):\n",
    "        super(Graph_Model, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.low_rank = low_rank\n",
    "        self.weight_graph_left = Parameter((1/self.in_features)*torch.ones(low_rank, in_features))\n",
    "        self.bias = Parameter(1e-12*torch.ones(in_features))    \n",
    "                \n",
    "    def forward(self, inputs):\n",
    "        self.weight_graph_left = self.weight_graph_left.to(args.device)\n",
    "        self.bias = self.bias.to(args.device)\n",
    "        inputs = inputs.to(args.device)\n",
    "        self.weight_graph_left.data = F.normalize(self.weight_graph_left, p=2, dim=1, eps=1e-10)\n",
    "        whole_graph = torch.abs(torch.matmul(self.weight_graph_left.transpose(0,1), self.weight_graph_left))\n",
    "        whole_graph = whole_graph.to(args.device)\n",
    "        whole_graph = whole_graph / whole_graph.sum(1, keepdim=True)\n",
    "        whole_graph = whole_graph * (1 - torch.eye(self.in_features, self.in_features).to(args.device))\n",
    "        whole_graph = whole_graph.to(args.device)\n",
    "        return (F.linear(inputs, whole_graph.transpose(0,1), bias = self.bias))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AAFL framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAFL(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AAFL, self).__init__()\n",
    "        self.m = args.m\n",
    "        self.low_rank = args.low_rank\n",
    "        self.w = args.window\n",
    "        self.batch_size = args.batch_size\n",
    "        self.scale_alpha = args.scale_alpha\n",
    "        self.p_list = args.p_list\n",
    "        self.p_allsum = np.sum(self.p_list)\n",
    "        self.len_p_list = len(self.p_list)\n",
    "        self.compress_p_list = args.compress_p_list\n",
    "        self.len_compress_p_list = len(self.compress_p_list)      \n",
    "        self.num_cluster = args.num_cluster\n",
    "        \n",
    "        self.linears = [ (nn.Linear(self.w, self.p_list[0]))]; #w->hid\n",
    "        if self.len_p_list>1:\n",
    "            for p_i in np.arange(1,self.len_p_list):\n",
    "                self.linears.append( (nn.Linear(self.p_list[p_i-1], self.p_list[p_i], bias=True))); #w->hid\n",
    "        ## graph layers\n",
    "        for cluster_i in range(self.num_cluster):\n",
    "            #for graph_i in range(self.num_graphs):\n",
    "            self.linears.append( Graph_Model(self.m, self.low_rank)); #m->m, supervised           \n",
    "            for com_i in range(self.len_compress_p_list):\n",
    "                if com_i == 0:\n",
    "                    in_fea = self.p_allsum\n",
    "                else:\n",
    "                    in_fea = self.compress_p_list[com_i-1]\n",
    "                self.linears.append( (nn.Linear(in_fea, self.compress_p_list[com_i], bias=True))) \n",
    "                \n",
    "            self.linears.append(FR_Model(args)); #k->k          \n",
    "        self.linears = nn.ModuleList(self.linears);\n",
    "        self.dropout = nn.Dropout(args.dropout);\n",
    "\n",
    "        for p_i in np.arange(0,self.len_p_list):           \n",
    "            nn.init.normal_(self.linears[p_i].weight, mean=0.0, std=1e-10)\n",
    "            nn.init.normal_(self.linears[p_i].bias, mean=0.0, std=1e-10)\n",
    "                \n",
    "    def forward(self, inputs):  \n",
    "        inputs  = inputs.to(args.device)\n",
    "        x_org = inputs.clone()\n",
    "        x_org = x_org.to(args.device)\n",
    "        x_p = []\n",
    "        x_0n = (x_org).repeat(1,1,self.p_list[0])\n",
    "        x_0n = x_0n.to(args.device)\n",
    "        x_0 = x_org.clone()\n",
    "        x_0 = x_0.to(args.device)\n",
    "        for layer_i in range(self.len_p_list):  \n",
    "            x_i = self.linears[layer_i](x_0);\n",
    "            x_i = F.relu(x_i + x_0n)\n",
    "            x_0n = x_i\n",
    "            x_0 = x_i\n",
    "            x_p.append(x_i)\n",
    "        \n",
    "        x_p_all = torch.cat(x_p, dim = 2) \n",
    "        x_p_all = self.dropout(x_p_all)\n",
    "        x_p_all = x_p_all.to(args.device)\n",
    "        \n",
    "        final_y_cluster = [[] for idx_class in range(self.num_cluster)] \n",
    "        for cluster_i in range(self.num_cluster):\n",
    "            x_sp =  x_p_all.transpose(2,1).contiguous(); ## read the data piece  \n",
    "            x_sp = x_sp.to(args.device)\n",
    "            x_sp = self.linears[self.len_p_list+cluster_i*(2+self.len_compress_p_list)+0](x_sp);  #lxk \n",
    "            x_sp = F.tanh(x_sp/self.scale_alpha);\n",
    "            x_sp = self.dropout(x_sp)\n",
    "            \n",
    "            x_sp = x_sp.transpose(2,1).contiguous(); #mxl\n",
    "            \n",
    "            for com_i in range(self.len_compress_p_list):\n",
    "                x_sp = self.linears[self.len_p_list+cluster_i*(2+self.len_compress_p_list)+1+com_i](x_sp);  #lxk \n",
    "                x_sp = F.tanh(x_sp/self.scale_alpha);\n",
    "                x_sp = self.dropout(x_sp)\n",
    "\n",
    "            x_sp = self.linears[self.len_p_list+cluster_i*(2+self.len_compress_p_list)+1+self.len_compress_p_list](x_sp); #mx2\n",
    "            x_sp = x_sp.to(args.device)\n",
    "            final_y_cluster[cluster_i] = (x_sp).squeeze().to(args.device)\n",
    "\n",
    "        return final_y_cluster      \n",
    "\n",
    "    def predict_relationship_inside(self):        \n",
    "        G_all = []\n",
    "        fea_weight_all = []  \n",
    "        Left_all  = []\n",
    "        Right_all = [] \n",
    "        Final_all = []       \n",
    "        for cluster_i in range(self.num_cluster):\n",
    "            Left = self.linears[self.len_p_list+cluster_i*(2+self.len_compress_p_list)+0].weight_graph_left.transpose(0,1)#.detach()\n",
    "            Right = self.linears[self.len_p_list+cluster_i*(2+self.len_compress_p_list)+0].weight_graph_left\n",
    "            Left = Left.to(args.device)\n",
    "            Right = Right.to(args.device)\n",
    "            \n",
    "            A = torch.matmul(Left, Right)\n",
    "            A = A.to(args.device)\n",
    "            A_nodiag = torch.abs(A * (1 - torch.eye(A.shape[0], A.shape[1]).to(args.device)))\n",
    "            A_nodiag = A_nodiag.to(args.device)\n",
    "            G_all.append(torch.abs(A_nodiag))#.detach().numpy())\n",
    "            \n",
    "            Left_all.append(torch.abs(Left))\n",
    "            Right_all.append(torch.abs(Right))\n",
    "                       \n",
    "            final_layer = (self.linears[self.len_p_list+cluster_i*(2+self.len_compress_p_list)+1+self.len_compress_p_list].weight[:,:,0])\n",
    "            final_layer = final_layer.to(args.device)\n",
    "            Final_all.append(final_layer)\n",
    "            \n",
    "            tmp = F.normalize(Left, p=2, dim=0, eps=1e-10).to(args.device)\n",
    "            tmp = torch.sum(torch.abs(tmp), dim=1).to(args.device) #+ torch.sum(torch.abs(Right), dim=0)\n",
    "            tmp = 1- F.normalize(tmp, p=2, dim=0, eps=1e-10).to(args.device)\n",
    "            tmp = F.normalize(tmp, p=2, dim=0, eps=1e-10).to(args.device)\n",
    "            fea_weight_all.append(tmp)\n",
    "            \n",
    "        return G_all, fea_weight_all, Left_all, Right_all, Final_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_permutation(n_clusters, real_labels, labels):\n",
    "    permutation=[]\n",
    "    for i in range(n_clusters):\n",
    "        idx = labels == i\n",
    "        new_label=scipy.stats.mode(real_labels[idx])[0][0]  # Choose the most common label among data points in the cluster\n",
    "        permutation.append(new_label)\n",
    "    return permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, model, criterion, optim, batch_size):\n",
    "    model.train();\n",
    "    total_loss = 0;\n",
    "    n_samples = 0;       \n",
    "    total_time = 0\n",
    "    counter = 0    \n",
    "    \n",
    "    for inputs in get_batches(data, batch_size, True):     \n",
    "        begin_time1 = time.time()\n",
    "        X, Y = inputs[0], inputs[1]\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)  \n",
    "        model.zero_grad();\n",
    "        output = model(X);  \n",
    "        \n",
    "        G_all, fea_scaler, Left_all, Right_all, Final_all = model.predict_relationship_inside()\n",
    "        residuals_weighted_agg_batch = torch.zeros((len(Y), len(output))).to(args.device)\n",
    "        residuals_weighted_agg_batch = residuals_weighted_agg_batch.to(args.device)\n",
    "        for cluster_i in range(model.num_cluster):\n",
    "            residuals_tmp = criterion(output[cluster_i], Y) ### raw residuals: bxm\n",
    "            residuals_tmp = residuals_tmp.to(args.device)\n",
    "            weight_tmp = fea_scaler[cluster_i][None, :].repeat(len(Y),1) ### weighting vector: bxm\n",
    "            weight_tmp = weight_tmp.to(args.device)\n",
    "            residuals_tmp = torch.mul(residuals_tmp, weight_tmp)  ## weighting\n",
    "            residuals_tmp = residuals_tmp.to(args.device)\n",
    "            residuals_weighted_agg_batch[:, cluster_i] = torch.sum( residuals_tmp, dim=1).to(args.device);   ### sum_residual: b \n",
    "   \n",
    "        loss_org = torch.sum(torch.min(residuals_weighted_agg_batch, dim=1)[0])   \n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1e-30)  \n",
    "               \n",
    "        loss_org.backward(retain_graph=True)\n",
    "        total_loss += loss_org.data.item();\n",
    "        optim.step();\n",
    "        #n_samples += output[0].size(0) * output[0].size(1); \n",
    "        n_samples += args.batch_size * args.m\n",
    "        counter = counter + 1        \n",
    "        total_time = total_time + time.time() - begin_time1        \n",
    "\n",
    "    return total_loss / n_samples, total_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data, model, criterion, batch_size):\n",
    "    total_loss = 0;\n",
    "    n_samples = 0;       \n",
    "    total_time = 0\n",
    "    counter = 0\n",
    "    labels_predict = []             ## predict clustering label\n",
    "    residuals_weighted_AGG = []     ## record all weighted residuals aggregated (no feature side)\n",
    "    residuals_raw_Full = []         ## record all weighted residuals \n",
    "    residuals_weighted_FULL = []    ## record all weighted residuals full (with feature side)\n",
    "    predict_FULL = []               ## ????\n",
    "    \n",
    "    for inputs in get_batches(data, batch_size, False):     \n",
    "        begin_time1 = time.time()\n",
    "        X, Y = inputs[0], inputs[1]\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        output = model(X);        \n",
    "        G_all, fea_scaler, Left_all, Right_all, Final_all  = model.predict_relationship_inside()\n",
    "        residuals_weighted_agg_batch = torch.zeros((len(Y), len(output))).to(args.device)                       ## bxc\n",
    "        residuals_weighted_agg_batch = residuals_weighted_agg_batch.to(args.device)\n",
    "        residuals_raw_batch = torch.zeros((len(Y), X.shape[1], len(output))).to(args.device)                    ## bxmxc\n",
    "        residuals_raw_batch = residuals_raw_batch.to(args.device)\n",
    "        residuals_weighted_batch =  torch.zeros((len(Y), X.shape[1], len(output))).to(args.device)              ## bxmxc\n",
    "        residuals_weighted_batch = residuals_weighted_batch.to(args.device) \n",
    "        predict_FULL_batch =  torch.zeros((len(Y), X.shape[1], len(output))).to(args.device)                    ## bxmxc\n",
    "        predict_FULL_batch = predict_FULL_batch.to(args.device) \n",
    "        for cluster_i in range(model.num_cluster):\n",
    "            predict_FULL_batch[:, :, cluster_i] = output[cluster_i]                             ## record raw prediction: bxm(xc)\n",
    "            residuals_tmp = criterion(output[cluster_i], Y)                                     ## raw residuals: bxm\n",
    "            residuals_tmp = residuals_tmp.to(args.device)      \n",
    "            residuals_raw_batch[:, :, cluster_i] = residuals_tmp                                ## record raw residual: bxm(xc)\n",
    "            weight_tmp = fea_scaler[cluster_i][None, :].repeat(len(Y),1)                        ## weighting: bxm\n",
    "            weight_tmp = weight_tmp.to(args.device)\n",
    "            residuals_tmp = torch.mul(residuals_tmp, weight_tmp).to(args.device)                                ## weighting\n",
    "            residuals_tmp = residuals_tmp.to(args.device)\n",
    "            residuals_weighted_batch[:,:,cluster_i] = residuals_tmp                             ## record weigthed residual: bxm(xc)\n",
    "            residuals_weighted_agg_batch[:, cluster_i] = torch.sum( residuals_tmp, dim=1);      ## record aggregated weighted residual: b(xc)  \n",
    "\n",
    "        residuals_weighted_AGG.append(residuals_weighted_agg_batch)                             ## record aggregated weighted residual: bxc   \n",
    "        residuals_raw_Full.append(residuals_raw_batch)                                          ## record raw residual: bxmxc\n",
    "        residuals_weighted_FULL.append(residuals_weighted_batch)                                ## record weigthed residual: bxmxc\n",
    "        predict_FULL.append(predict_FULL_batch)                                                 ## record raw prediction: bxmxc\n",
    "    \n",
    "        loss_org = torch.sum(torch.min(residuals_weighted_agg_batch, dim=1)[0])                 ## sum of min (bxc -> b -> 1)     \n",
    "        labels_predict.append(torch.min(residuals_weighted_agg_batch, dim=1)[1])                ## label from min (bxc): bx1 \n",
    "        \n",
    "        total_loss += loss_org.data.item();\n",
    "        #n_samples += (output[0].size(0) * output[0].size(1)); \n",
    "        n_samples += args.batch_size * args.m \n",
    "        counter = counter + 1\n",
    "        total_time = total_time + time.time() - begin_time1\n",
    "    \n",
    "    labels_predict = torch.cat(labels_predict, dim = 0)                                         ## label: nx1  \n",
    "    labels_predict = labels_predict.cpu()\n",
    "    predict_FULL = (torch.cat(predict_FULL, dim = 0)).detach().cpu().numpy()                          ## record raw prediction: nxmxc\n",
    "    residuals_raw_Full = (torch.cat(residuals_raw_Full, dim = 0)).detach().cpu().numpy()              ## record raw residual: nxmxc\n",
    "    residuals_weighted_FULL = (torch.cat(residuals_weighted_FULL, dim = 0)).detach().cpu().numpy()    ## record weigthed residual: nxmxc   \n",
    "    residuals_weighted_AGG = (torch.cat(residuals_weighted_AGG, dim = 0)).detach().cpu().numpy()      ## record aggregated weighted residual: nxc   \n",
    "    \n",
    "    residual_min_tmp = np.repeat(np.min(residuals_weighted_AGG, axis=1)[:, np.newaxis], model.num_cluster, axis=1)\n",
    "    residuals_weighted_AGG = residuals_weighted_AGG - residual_min_tmp                                          \n",
    "    residual_max_tmp = np.repeat(np.max(residuals_weighted_AGG, axis=1)[:, np.newaxis], model.num_cluster, axis=1)\n",
    "    residuals_weighted_AGG = np.divide(residuals_weighted_AGG, residual_max_tmp)\n",
    "       \n",
    "    return total_loss/n_samples, total_time, labels_predict, predict_FULL, residuals_raw_Full, residuals_weighted_FULL, residuals_weighted_AGG, G_all, fea_scaler, Left_all, Right_all, Final_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2(data, model, criterion, batch_size,labels_predict_):\n",
    "    counter = 0            \n",
    "    labels_predict = labels_predict_ ## predict clustering label\n",
    "    reconstruction  = []\n",
    "  \n",
    "    for inputs in get_batches(data, batch_size, False):     \n",
    "        X = inputs[0]\n",
    "        X = X.to(args.device)\n",
    "        output = model(X);\n",
    "        \n",
    "        num_cluster = labels_predict[counter]\n",
    "        counter += 1\n",
    "        \n",
    "        n = output[num_cluster].detach().cpu().numpy()\n",
    "        reconstruction.append(n)\n",
    "\n",
    "    return reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test3(data, model, criterion, batch_size,labels_predict_, Left_all):\n",
    "    counter = 0            \n",
    "    labels_predict = labels_predict_ ## predict clustering label\n",
    "    reconstruction  = []\n",
    "    weight = []\n",
    "    weight.append(Left_all[0].squeeze().detach().cpu().numpy())\n",
    "    weight.append(Left_all[1].squeeze().detach().cpu().numpy())\n",
    "    \n",
    "    for inputs in get_batches(data, batch_size, False):     \n",
    "        X_ = inputs[0]\n",
    "        X = X_\n",
    "        X = X.to(args.device)\n",
    "        output = model(X);\n",
    "        \n",
    "        num_cluster = labels_predict[counter]\n",
    "        counter += 1\n",
    "        \n",
    "        x = []\n",
    "        X_ = X_.squeeze().numpy()\n",
    "        for i in range(args.m) :\n",
    "            if X_[i] == 0 : ## imputation\n",
    "                imp = 0\n",
    "                w = weight[num_cluster] # weight\n",
    "                n = output[num_cluster].detach().cpu().numpy() # generated values\n",
    "                \n",
    "                for j in range(args.m):\n",
    "                    if X_[j] == 0 and i != j :\n",
    "                        imp += w[j] * n[j]\n",
    "                    if X_[j] != 0 :\n",
    "                        imp += w[j] * X_[j]\n",
    "                \n",
    "                x.append(imp)\n",
    "                \n",
    "            else:# raw values\n",
    "                x.append(X_[i])       \n",
    "        \n",
    "        reconstruction.append(x)\n",
    "\n",
    "    return reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size, shuffle = False):    \n",
    "    inputs = data[0]\n",
    "    targets = data[1]\n",
    "    length = len(inputs)\n",
    "    if shuffle:\n",
    "        index = torch.randperm(length)\n",
    "    else:\n",
    "        index = torch.LongTensor(range(length))\n",
    "    start_idx = 0\n",
    "        \n",
    "    while (start_idx < length):\n",
    "        end_idx = min(length, start_idx + batch_size)\n",
    "        excerpt = index[start_idx:end_idx]\n",
    "        X = inputs[excerpt]; \n",
    "        Y = targets[excerpt];              \n",
    "        data = [Variable(X), Variable(Y)]\n",
    "        yield data;\n",
    "        start_idx += batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameter set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    scale_alpha = 1\n",
    "    window = 1\n",
    "    pre_win = 1\n",
    "    low_rank = 1\n",
    "\n",
    "    p_list = [10]*10\n",
    "    compress_p_list = [10,1]\n",
    "    L1Loss = False#False\n",
    "    clip = 1.\n",
    "    epochs = 100 # 100\n",
    "    batch_size = 100\n",
    "    dropout = 0.00001\n",
    "    seed = 12345\n",
    "    gpu = 0\n",
    "    optim = 'adamW'#'adam'\n",
    "    #lr = 0.000001\n",
    "    lr = 1e-3#1e-7\n",
    "    weight_decay = 5e-5#lr/10.\n",
    "    horizon = 1\n",
    "    \n",
    "    random_shuffle = True\n",
    "    train = True # True or False\n",
    "    test = True # True or False\n",
    "    disable_cuda = True\n",
    "    random_seed = 123\n",
    "    device = None\n",
    "    save_path = './result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buliding model\n",
      "0.001\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=1, out_features=10, bias=True)\n",
      "  (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (5): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (6): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (7): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (8): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (9): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (10): Graph_Model()\n",
      "  (11): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (12): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (13): FR_Model()\n",
      "  (14): Graph_Model()\n",
      "  (15): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (16): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (17): FR_Model()\n",
      ")\n",
      "* number of parameters: 12412\n",
      "data org shape:  torch.Size([4645, 1170])\n"
     ]
    }
   ],
   "source": [
    "# Load preproceed dataset\n",
    "args = args\n",
    "random.seed(args.random_seed)\n",
    "np.random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)\n",
    "\n",
    "if args.disable_cuda and torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed(args.random_seed)\n",
    "else:\n",
    "    args.device = torch.device('cpu')\n",
    "\n",
    "if args.L1Loss:\n",
    "    criterion = nn.L1Loss(size_average=False,reduce=False);\n",
    "else:\n",
    "    criterion = nn.MSELoss(size_average=True,reduce=False); #SmoothL1Loss    \n",
    "\n",
    "best_val = 10000000;\n",
    "print('buliding model')\n",
    "    \n",
    "print(args.lr)\n",
    "\n",
    "# data path\n",
    "filename   = './data/GSE72056_melanoma_single_cell_revised_v2_gene_filtering_ben3388mal1257_deg.csv'\n",
    " \n",
    "with open(filename,encoding = 'utf-8') as f:\n",
    "    data_org = np.loadtxt(f,str,delimiter = \",\")\n",
    "truth_labels = np.array(data_org[1,1:],dtype='uint8')\n",
    "data_org = data_org[2:,1:].T\n",
    "data_org = data_org.astype(np.float)#.toarray()\n",
    "#data_org = normalize(data_org, norm='l2')\n",
    "    \n",
    "args.m = data_org.shape[1]\n",
    "args.num_cluster = 2 \n",
    "    \n",
    "truth_labels = truth_labels.squeeze()\n",
    "\n",
    "model = AAFL(args);\n",
    "model = model.to(args.device)\n",
    "optim = Optim(\n",
    "    model.parameters(), args.optim, args.lr, args.clip, weight_decay = args.weight_decay,\n",
    "    )\n",
    "\n",
    "print(model.linears)\n",
    "nParams = sum([p.nelement() for p in model.parameters()])\n",
    "print('* number of parameters: %d' % nParams)\n",
    "\n",
    "data_org_tensor = torch.from_numpy(data_org).float()\n",
    "train_data_org = [ data_org_tensor[:, :, None],  data_org_tensor]\n",
    "train_data = train_data_org.copy()\n",
    "test_data = [ data_org_tensor[:, :, None],  data_org_tensor]\n",
    "\n",
    "print('data org shape: ', data_org_tensor.shape)\n",
    "\n",
    "train_loss_all = []\n",
    "test_loss_all = []\n",
    "NMI_all = []\n",
    "ACC_all = []        \n",
    "\n",
    "test_truth_labels = truth_labels\n",
    "\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.makedirs(args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "if args.train == True:\n",
    "    print (\"1-train\")\n",
    "    for epoch in range(args.epochs): \n",
    "        train_loss, epoch_time = train(train_data, model, criterion, optim, args.batch_size)  \n",
    "        torch.save(model.state_dict(), os.path.join(args.save_path, 'epoch_%d_num_cluster_%d_best.model' % (epoch, args.num_cluster)))\n",
    "        torch.save(model.state_dict(), os.path.join(args.save_path, 'best.model'))\n",
    "\n",
    "        #now = datetime.datetime.now()\n",
    "        #print ('Epoch: ', epoch, ' ', now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        #print('  train_loss:', \"{:5.7f}\".format(train_loss))\n",
    "        #train_loss_all.append(train_loss) \n",
    "    \n",
    "        test_loss, epoch_time, labels_predict, predict_FULL, residuals_raw_Full, residuals_weighted_FULL, residuals_weighted_AGG, G_all, fea_scaler, Left_all, Right_all, Final_all = test(test_data, model, criterion, args.batch_size)\n",
    "        labels_predict = labels_predict.detach().numpy() \n",
    "    \n",
    "        AAFL_NMI = normalized_mutual_info_score(test_truth_labels, labels_predict)    \n",
    "        if len(np.unique(labels_predict))>=args.num_cluster:\n",
    "            permutation = find_permutation(args.num_cluster, test_truth_labels, labels_predict)\n",
    "            new_labels = [ permutation[label] for label in labels_predict]   # permute the labels\n",
    "            AAFL_ACC = accuracy_score(test_truth_labels, new_labels)   \n",
    "            permutation_reorder = np.argsort(permutation)\n",
    "        else:\n",
    "            AAFL_ACC = -1\n",
    "            permutation_reorder = range(args.num_cluster)\n",
    "    \n",
    "        now = datetime.datetime.now()\n",
    "        print ('Epoch: ', epoch, ' ', now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        print('AAFL_NMI:', \"{:5.3f}\".format(AAFL_NMI), ' AAFL_ACC:', \"{:5.3f}\".format(AAFL_ACC), ' trn_loss:', \"{:5.7f}\".format(train_loss), ' tst_loss:', \"{:5.7f}\".format(test_loss))\n",
    "        train_loss_all.append(train_loss) \n",
    "        test_loss_all.append(test_loss)  \n",
    "        NMI_all.append(AAFL_NMI)\n",
    "        ACC_all.append(AAFL_ACC)        \n",
    "else:\n",
    "    print (\"2-test\")\n",
    "    model.load_state_dict(torch.load(os.path.join(args.save_path, \"best.model\")))\n",
    "        \n",
    "    batch_size = 100\n",
    "    test_loss, epoch_time, labels_predict, predict_FULL, residuals_raw_Full, residuals_weighted_FULL, residuals_weighted_AGG, G_all, fea_scaler, Left_all, Right_all, Final_all = test(test_data, model, criterion, batch_size)\n",
    "    labels_predict = labels_predict.detach().numpy()   \n",
    "    print(labels_predict)\n",
    "        \n",
    "    labels_predict_ = labels_predict + 1\n",
    "    print(labels_predict_)\n",
    "    print(test_truth_labels)\n",
    "        \n",
    "    AAFL_NMI = normalized_mutual_info_score(test_truth_labels, labels_predict)    \n",
    "    if len(np.unique(labels_predict))>=args.num_cluster:\n",
    "        permutation = find_permutation(args.num_cluster, test_truth_labels, labels_predict)\n",
    "        new_labels = [ permutation[label] for label in labels_predict]   # permute the labels\n",
    "        AAFL_ACC = accuracy_score(test_truth_labels, new_labels)   \n",
    "        permutation_reorder = np.argsort(permutation)\n",
    "    else:\n",
    "        AAFL_ACC = -1\n",
    "        permutation_reorder = range(args.num_cluster)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    print('AAFL_NMI:', \"{:5.3f}\".format(AAFL_NMI), ' AAFL_ACC:', \"{:5.3f}\".format(AAFL_ACC))\n",
    "    NMI_all.append(AAFL_NMI)\n",
    "    ACC_all.append(AAFL_ACC)        \n",
    "                       \n",
    "    fea_weight_0 = fea_scaler[0].detach().cpu().numpy()\n",
    "    np.savetxt('./result/feature_weight_0_n2_label.csv', fea_weight_0, delimiter = ',')\n",
    " \n",
    "    fea_weight_1 = fea_scaler[1].detach().cpu().numpy()\n",
    "    np.savetxt('./result/feature_weight_1_n2_label.csv', fea_weight_1, delimiter = ',')\n",
    "        \n",
    "    left_0 = Left_all[0].detach().cpu().numpy()\n",
    "    np.savetxt('./result/left_0_n2_label.csv', left_0, delimiter = ',')\n",
    "\n",
    "    left_1 = Left_all[1].detach().cpu().numpy()\n",
    "    np.savetxt('./result/left_1_n2_label.csv', left_1, delimiter = ',')\n",
    "       \n",
    "    print('Left_all: ', Left_all)        \n",
    "        \n",
    "    np.savetxt('./result/labels_predict_n2_label.csv', labels_predict_, delimiter = ',')\n",
    "        \n",
    "    print(' tst_loss:', \"{:5.7f}\".format(test_loss))\n",
    "    test_loss_all.append(test_loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mdoel\n",
    "if args.test == True:\n",
    "        \n",
    "    new_model = KMeans(n_clusters=model.num_cluster, n_init=50, random_state=np.random.randint(0, 500), tol = 1e-10)\n",
    "    kmeans = new_model.fit(data_org)\n",
    "    kmeans_labels = np.copy(kmeans.labels_)\n",
    "    if len(np.unique(kmeans_labels))>=args.num_cluster:\n",
    "        permutation = find_permutation(args.num_cluster, truth_labels, kmeans_labels)\n",
    "        new_labels = [ permutation[label] for label in kmeans_labels]   # permute the labels\n",
    "        min_res_ACC = accuracy_score(truth_labels, new_labels)   \n",
    "        permutation_reorder = np.argsort(permutation)\n",
    "    else:\n",
    "        min_res_ACC = -1\n",
    "        permutation_reorder = range(args.num_cluster)\n",
    "    print('simple Kmeans_NMI:', normalized_mutual_info_score(truth_labels, kmeans_labels))\n",
    "    print('simple Kmeans_ACC:', min_res_ACC)\n",
    "\n",
    "\n",
    "    GMM_model = mixture.GaussianMixture(n_components=model.num_cluster, reg_covar=1e-11)\n",
    "    GMM_model.fit(data_org) \n",
    "    GMM_labels = GMM_model.predict(data_org)\n",
    "    if len(np.unique(GMM_labels))>=args.num_cluster:\n",
    "        permutation = find_permutation(args.num_cluster, truth_labels, GMM_labels)\n",
    "        new_labels = [ permutation[label] for label in GMM_labels]   # permute the labels\n",
    "        min_res_ACC = accuracy_score(truth_labels, new_labels)   \n",
    "        permutation_reorder = np.argsort(permutation)\n",
    "    else:\n",
    "        min_res_ACC = -1\n",
    "        permutation_reorder = range(args.num_cluster)\n",
    "    print('simple GMM_NMI:', normalized_mutual_info_score(truth_labels, GMM_labels))\n",
    "    print('simple GMM_ACC:', min_res_ACC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
